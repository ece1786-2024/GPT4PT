{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873527b9-88dd-436a-8e1f-377416405cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing tokenizer from: tokenizer\n",
      "Loading existing model from: EI\n",
      "Introverted: 50.0%, Extroverted: 50.0%\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question: Do you feel more energized after spending time with a group of people or after having some time alone?\n",
      " I feel more energized after spending time with a group of people but only if it's not too long\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introverted: 0.5266889464110136%, Extroverted: 99.47330951690674%\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question: Do you prefer spending time in large groups, or do you find one-on-one interactions more fulfilling?\n",
      " neither of these two, I prefer spending time in a small group\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introverted: 0.5953083280473948%, Extroverted: 99.40469264984131%\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question: Do you feel more energized after spending time with a large group of people?\n",
      " no I don't\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from openai import OpenAI\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class EiClassifier:\n",
    "    '''\n",
    "    Prerequisite:\n",
    "    1. A `tokenizer` folder under the same directory to be loaded by `AutoTokenizer`\n",
    "    2. A `model` folder under the same directory to be loaded by `AutoModelForSequenceClassification`\n",
    "    3. `OPENAI_API_KEY` set in `os.environ`\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.labels = {0: \"E\", 1: \"I\"}\n",
    "        self.tokenizer = None\n",
    "        self.gpt2 = None\n",
    "        self.gpt4 = OpenAI()\n",
    "        \n",
    "        self._load_tokenizer()\n",
    "        self._load_gpt2()\n",
    "\n",
    "    def _load_tokenizer(self):\n",
    "        tokenizer_path = \"tokenizer\"\n",
    "        assert(os.path.exists(tokenizer_path))\n",
    "\n",
    "        print(\"Loading existing tokenizer from:\", tokenizer_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "    def _load_gpt2(self):\n",
    "        model_path = 'EI'\n",
    "        assert(os.path.exists(model_path))\n",
    "\n",
    "        print(\"Loading existing model from:\", model_path)\n",
    "        self.gpt2 = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "    def _init_gpt4(self):\n",
    "        prompt_path = 'prompt.txt'\n",
    "        #assert(['OPENAI_API_KEY'] in os.environ)\n",
    "        assert 'OPENAI_API_KEY' in os.environ, \"OPENAI_API_KEY is not set in the environment variables\"\n",
    "        assert(os.path.exists(prompt_path))\n",
    "\n",
    "        \n",
    "        with open(prompt_path, 'r') as file:\n",
    "            prompt = file.read()\n",
    "        # some api version doens't support memory, so no need to init.\n",
    "        #    #print(prompt)\n",
    "        #    _ = self.gpt4.chat.completions.create(\n",
    "        #        model=\"gpt-4o\",\n",
    "        #        messages=[\n",
    "        #            {\"role\": \"system\", \"content\": prompt},\n",
    "        #        ]\n",
    "        #    )\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def _to_probs(self, logits: torch.Tensor, dim=-1):\n",
    "        max_logits = torch.max(logits, dim=dim, keepdim=True).values\n",
    "        shifted_logits = logits - max_logits\n",
    "        exp_logits = torch.exp(shifted_logits)\n",
    "        probs = exp_logits / torch.sum(exp_logits, dim=dim, keepdim=True)\n",
    "        return probs.tolist()[0]\n",
    "\n",
    "    def classify(self, text: str):\n",
    "        encoded_input = self.tokenizer(text, truncation=True, padding=True, max_length=100, return_tensors=\"pt\")\n",
    "        outputs = self.gpt2(**encoded_input)\n",
    "        logits = outputs.logits\n",
    "        return self._to_probs(logits)\n",
    "    \n",
    "    def _generate_question(self, prompt, probs: list):\n",
    "        print(f'Introverted: {probs[0]*100}%, Extroverted: {probs[1]*100}%')\n",
    "        return self.gpt4.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"system\", \"content\": prompt}] + \n",
    "            [{\"role\": \"user\", \"content\": f'Introverted: {probs[0]*100}%, Extroverted: {probs[1]*100}%'}],\n",
    "            temperature=0.7,\n",
    "            #temperature=0.5,\n",
    "            top_p=1\n",
    "        ).choices[0].message.content\n",
    "\n",
    "    def start(self):\n",
    "        responses = []\n",
    "        prompt = self._init_gpt4()\n",
    "        # First question start with 0% and 0%\n",
    "        question = self._generate_question(prompt, [0.5,0.5])\n",
    "        answer = input(f'Question: {question}\\n')\n",
    "        responses.append(question)\n",
    "        responses.append(answer)\n",
    "        probs = self.classify('\\n'.join(responses))\n",
    "        #print(probs)\n",
    "        \n",
    "        # Second question uses the probs from last question\n",
    "        question = self._generate_question(prompt, probs)\n",
    "        answer = input(f'Question: {question}\\n')\n",
    "        responses.append(question)\n",
    "        responses.append(answer)\n",
    "        probs = self.classify('\\n'.join(responses))\n",
    "        #print(probs)\n",
    "        \n",
    "        # Third question uses the probs from last question\n",
    "        question = self._generate_question(prompt, probs)\n",
    "        answer = input(f'Question: {question}\\n')\n",
    "        responses.append(question)\n",
    "        responses.append(answer)\n",
    "        probs = self.classify('\\n'.join(responses))\n",
    "        #print(probs)\n",
    "        \n",
    "        return 'E' if probs[1] > probs[0] else 'I'\n",
    "\n",
    "c2 = EiClassifier().start()\n",
    "print(c2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
