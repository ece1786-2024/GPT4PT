{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/ece1786/project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ks5vbsYRshs",
        "outputId": "21ec7507-f180-4658-8149-68c6391080b0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/ece1786/project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from openai import OpenAI\n",
        "import torch\n",
        "import os\n",
        "\n",
        "class JpClassifier:\n",
        "    '''\n",
        "    Prerequisite:\n",
        "    1. A `tokenizer` folder under the same directory to be loaded by `AutoTokenizer`\n",
        "    2. A `model` folder under the same directory to be loaded by `AutoModelForSequenceClassification`\n",
        "    3. `OPENAI_API_KEY` set in `os.environ`\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        self.labels = {0: \"J\", 1: \"P\"}\n",
        "        self.tokenizer = None\n",
        "        self.gpt2 = None\n",
        "        #self.gpt4 = OpenAI()\n",
        "        self.gpt4 = OpenAI(api_key='OPENAI_API_KEY')\n",
        "\n",
        "        self._load_tokenizer()\n",
        "        self._load_gpt2()\n",
        "\n",
        "    def _load_tokenizer(self):\n",
        "        tokenizer_path = \"cpt/tokenizer\"\n",
        "        assert(os.path.exists(tokenizer_path))\n",
        "\n",
        "        print(\"Loading existing tokenizer from:\", tokenizer_path)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "\n",
        "    def _load_gpt2(self):\n",
        "        model_path = 'cpt/model'\n",
        "        assert(os.path.exists(model_path))\n",
        "\n",
        "        print(\"Loading existing model from:\", model_path)\n",
        "        self.gpt2 = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "    def _init_gpt4(self):\n",
        "        prompt_path = 'prompt.txt'\n",
        "        os.environ['OPENAI_API_KEY'] = 'OPENAI_API_KEY'\n",
        "        # print(os.environ['OPENAI_API_KEY'])\n",
        "        assert('OPENAI_API_KEY' in os.environ)\n",
        "        assert(os.path.exists(prompt_path))\n",
        "\n",
        "\n",
        "        with open(prompt_path, 'r') as file:\n",
        "            prompt = file.read()\n",
        "        # some api version doens't support memory, so no need to init.\n",
        "        #    #print(prompt)\n",
        "        #    _ = self.gpt4.chat.completions.create(\n",
        "        #        model=\"gpt-4o\",\n",
        "        #        messages=[\n",
        "        #            {\"role\": \"system\", \"content\": prompt},\n",
        "        #        ]\n",
        "        #    )\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def _to_probs(self, logits: torch.Tensor, dim=-1):\n",
        "        max_logits = torch.max(logits, dim=dim, keepdim=True).values\n",
        "        shifted_logits = logits - max_logits\n",
        "        exp_logits = torch.exp(shifted_logits)\n",
        "        probs = exp_logits / torch.sum(exp_logits, dim=dim, keepdim=True)\n",
        "        return probs.tolist()[0]\n",
        "\n",
        "    def classify(self, text: str):\n",
        "        encoded_input = self.tokenizer(text, truncation=True, padding=True, max_length=100, return_tensors=\"pt\")\n",
        "        outputs = self.gpt2(**encoded_input)\n",
        "        logits = outputs.logits\n",
        "        return self._to_probs(logits)\n",
        "\n",
        "    def _generate_question(self, prompt, probs: list):\n",
        "        print(f'Perceiving: {probs[0]*100}%, Judging: {probs[1]*100}%')\n",
        "        return self.gpt4.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"system\", \"content\": prompt}] +\n",
        "            [{\"role\": \"user\", \"content\": f'Perceiving: {probs[0]*100}%, Judging: {probs[1]*100}%'}],\n",
        "            temperature=0.7,\n",
        "            #temperature=0.5,\n",
        "            top_p=1\n",
        "        ).choices[0].message.content\n",
        "\n",
        "    def start(self):\n",
        "        responses = []\n",
        "        prompt = self._init_gpt4()\n",
        "        # First question start with 0% and 0%\n",
        "        question = self._generate_question(prompt, [0.5,0.5])\n",
        "        answer = input(f'Question: {question}\\n')\n",
        "        responses.append(question)\n",
        "        responses.append(answer)\n",
        "        probs = self.classify('\\n'.join(responses))\n",
        "        #print(probs)\n",
        "\n",
        "        # Second question uses the probs from last question\n",
        "        question = self._generate_question(prompt, probs)\n",
        "        answer = input(f'Question: {question}\\n')\n",
        "        responses.append(question)\n",
        "        responses.append(answer)\n",
        "        probs = self.classify('\\n'.join(responses))\n",
        "        #print(probs)\n",
        "\n",
        "        # Third question uses the probs from last question\n",
        "        question = self._generate_question(prompt, probs)\n",
        "        answer = input(f'Question: {question}\\n')\n",
        "        responses.append(question)\n",
        "        responses.append(answer)\n",
        "        probs = self.classify('\\n'.join(responses))\n",
        "        #print(probs)\n",
        "\n",
        "        return 'J' if probs[1] > probs[0] else 'P'\n"
      ],
      "metadata": {
        "id": "LlldK_vnPQ1U"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c2 = JpClassifier().start()\n",
        "print(c2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7UkAXuvRqLb",
        "outputId": "c86f7370-e60c-4bcc-866e-df7971e72e4e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing tokenizer from: cpt/tokenizer\n",
            "Loading existing model from: cpt/model\n",
            "Perceiving: 50.0%, Judging: 50.0%\n",
            "Question: Do you prefer to have detailed plans before starting a project, or are you comfortable diving in and adapting as you go?\n",
            "detailed plans\n",
            "Perceiving: 46.59121334552765%, Judging: 53.40878963470459%\n",
            "Question: Do you prefer planning your day in advance or keeping your schedule flexible?\n",
            "plan in advance\n",
            "Perceiving: 50.88996887207031%, Judging: 49.110034108161926%\n",
            "Question: Do you prefer to keep your options open or stick to a plan once you've made it?\n",
            "stick to a plan\n",
            "J\n"
          ]
        }
      ]
    }
  ]
}